# ETL
Extract, transform, load
The Sources: Allsides.com, Guardian IP Tools: Google Cloud, Python, Juypter Notebook, SQL, Postgres The initial process was to identify the sources we would be using to collect our data. Once the data was obtained through web scrapping API’s, we cleaned and organized our data using Pandas in Juypter notebooks. Once the data was organized in an easy read format the last step was to transfer our final output into a Data Base. Extraction: We web scrapped data from Allsides.com and then collected data using pandas from The Guardian API, used a loop to save off the information from the API in a list, used lists to create base data frame and then took date in data frame and converted it to standard format using datetime. Transformation: We pulled in various data sources and loaded them into the data frames, we reviewed the files and transformed them into data frames into Article, Source, Category, and Sub Header. We did an initial connection to the Postgres database using PG admin to store our original clean data sets. We used the quick database website to create the initial table schema that got loaded into the Postgres database that generated the first set of tables. After running the queries and created the new tables with only the relevant information we reconnected to the database and generated additional tables for the data frames. Upload: The last step was to transfer our final output into a Database. We created a database and respective tables to match the columns from the final Panda’s Data Frame using SQL and then connected to the database using SQL Alchemy and loaded the result to Postgres. Here we were able to perform multiple queries to suit a desired criterion.
